{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Состав:\n",
    "    Андрей Муратов\n",
    "    Ирина Никитина\n",
    "    Александр Волобуев\n",
    "    Филипп\n",
    "    \n",
    "На финальный скоринг выставляем:\n",
    "    Random Forest (sumbission5.csv)\n",
    "    Log Regression (subm2.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "comp_train = pd.read_csv('train.csv')\n",
    "X_test = pd.read_csv('test.csv')\n",
    "\n",
    "# подгтовка train сета. удаляем id и раскладываем остальные текстовые признаки\n",
    "comp_train = comp_train.drop(['_id'], axis=1)\n",
    "comp_train = pd.get_dummies(comp_train, columns=['job', 'marital', 'default', 'education', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome'])\n",
    "# сохраняем id в отдельной таблице файле для сабмишна и также удаляем лишнее и раскладываем\n",
    "final_doc = pd.DataFrame()\n",
    "final_doc['_id'] = X_test['_id'] \n",
    "X_test = X_test.drop(['_id'], axis=1)\n",
    "X_test = pd.get_dummies(X_test, columns=['job', 'marital', 'default', 'education', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome'])\n",
    "\n",
    "# стандартно подготавливаем x и y\n",
    "y = comp_train['target']\n",
    "X = comp_train.drop(['target'], axis=1)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "X_train, X_train_test, y_train, y_train_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "main_tree = RandomForestClassifier(random_state=1) #with gini criterion score is 0.7239\n",
    "cross_val_score(main_tree, X_train, y_train, cv=5, scoring='roc_auc').mean() #дерево на макс глубине\n",
    "\n",
    "from scipy.stats import randint as randint\n",
    "from scipy.stats import uniform\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import GridSearchCV\n",
    "    from sklearn.cross_validation import RandomizedSearchCV\n",
    "    from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': randint(2, 50),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': randint(2, 10),\n",
    "    'min_samples_leaf': randint(5, 30),\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'max_features': list(range(1, X_train.shape[1]))}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, random_state=111, shuffle=True)\n",
    "\n",
    "model = RandomForestClassifier(random_state=111)\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=2000, n_jobs=-1,\n",
    "                                   cv=cv, scoring='roc_auc', random_state=111)\n",
    "\n",
    "# А дальше, просто .fit()\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "random_search.best_params_\n",
    "\n",
    "random_search.best_score_\n",
    "\n",
    "model = random_search.best_estimator_\n",
    "model.feature_importances_\n",
    "\n",
    "np.nonzero(model.feature_importances_ > 0)\n",
    "\n",
    "model2 = RandomForestClassifier(random_state=111, class_weight = None,\n",
    " criterion = 'entropy',\n",
    " max_depth = 9,\n",
    " max_features = 30,\n",
    " min_samples_leaf = 14,\n",
    " n_estimators = 41)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "y_predict_tree = model2.predict_proba(X_train_test)\n",
    "y_tree = model2.predict(X_train_test)\n",
    "\n",
    "print('roc_auc_score', roc_auc_score(y_train_test, y_predict_tree[:, 1]))\n",
    "print('accuracy_score', accuracy_score(y_train_test, y_tree))\n",
    "print('precision_score', precision_score(y_train_test, y_tree))\n",
    "print('recall_score', recall_score(y_train_test, y_tree))\n",
    "print('f1_score', f1_score(y_train_test, y_tree))\n",
    "\n",
    "X_test = pd.read_csv('test.csv')\n",
    "final_doc = pd.DataFrame()\n",
    "final_doc['_id'] = X_test['_id']\n",
    "X_test = X_test.drop(['_id'], axis=1)\n",
    "X_test = pd.get_dummies(X_test, columns=['job', 'marital', 'default', 'education', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome'])\n",
    "\n",
    "final_doc['target'] = model2.predict_proba(X_test)[:, 1]\n",
    "final_doc.to_csv('submission5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7281957297559938"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "comp_train = pd.read_csv('train.csv')\n",
    "X_test = pd.read_csv('test.csv')\n",
    "\n",
    "comp_train = pd.read_csv('train.csv')\n",
    "X_test = pd.read_csv('test.csv')\n",
    "\n",
    "# подгтовка train сета. удаляем id и раскладываем остальные текстовые признаки\n",
    "comp_train = comp_train.drop(['_id'], axis=1)\n",
    "comp_train = pd.get_dummies(comp_train, columns=['job', 'marital', 'default', 'education', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome'])\n",
    "# сохраняем id в отдельной таблице файле для сабмишна и также удаляем лишнее и раскладываем\n",
    "final_doc = pd.DataFrame()\n",
    "final_doc['_id'] = X_test['_id'] \n",
    "X_test = X_test.drop(['_id'], axis=1)\n",
    "X_test = pd.get_dummies(X_test, columns=['job', 'marital', 'default', 'education', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome'])\n",
    "\n",
    "y = comp_train['target']\n",
    "X = comp_train.drop(['target'], axis=1)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "X_train, X_train_test, y_train, y_train_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "main_tree = DecisionTreeClassifier(criterion='entropy', random_state=1) #with gini criterion score is 0.7239\n",
    "cross_val_score(main_tree, X_train, y_train, cv=5, scoring='roc_auc').mean() #дерево на макс глубине"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as randint\n",
    "from scipy.stats import uniform\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import GridSearchCV\n",
    "    from sklearn.cross_validation import RandomizedSearchCV\n",
    "    from sklearn.cross_validation import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': randint(2, 10),\n",
    "    'min_samples_leaf': randint(5, 10),\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'max_features': list(range(1, X_train.shape[1]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=5, random_state=111, shuffle=True),\n",
       "          error_score='raise',\n",
       "          estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=111,\n",
       "            splitter='best'),\n",
       "          fit_params=None, iid=True, n_iter=200, n_jobs=-1,\n",
       "          param_distributions={'criterion': ['gini', 'entropy'], 'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001310E396E10>, 'min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001310E3C70B8>, 'class_weight': [None, 'balanced'], 'max_features': [1, 2, 3...38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62]},\n",
       "          pre_dispatch='2*n_jobs', random_state=111, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=5, random_state=111, shuffle=True)\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=111)\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=200, n_jobs=-1,\n",
    "                                   cv=cv, scoring='roc_auc', random_state=111)\n",
    "# А дальше, просто .fit()\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': 5,\n",
       " 'max_features': 56,\n",
       " 'min_samples_leaf': 9}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9352086816510002"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00253191, 0.42926627, 0.00134076, 0.0068908 , 0.00132056,\n",
       "       0.26251267, 0.01263193, 0.01326874, 0.05040853, 0.15755752,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.001492  , 0.        ,\n",
       "       0.        , 0.        , 0.00127478, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00092116, 0.00198047, 0.00338965, 0.00366086,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.02771981, 0.        ,\n",
       "       0.00097294, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.00178543, 0.00134611, 0.01772712])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = random_search.best_estimator_\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 18, 22, 41, 42, 43, 44, 53,\n",
       "        55, 60, 61, 62], dtype=int64),)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(model.feature_importances_ > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
       "            max_depth=5, max_features=56, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=9, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=111,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = DecisionTreeClassifier(random_state=111, class_weight = 'balanced',\n",
    " criterion = 'entropy',\n",
    " max_depth = 5,\n",
    " max_features = 56,\n",
    " min_samples_leaf = 9)\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_tree = model2.predict_proba(X_train_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9336412110339083"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_train_test, y_predict_tree[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_doc['target'] = model2.predict_proba(X_test)[:, 1]\n",
    "final_doc.to_csv('subm3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Pipeline([\n",
    "#    ('vect', StandardScaler()),\n",
    "#    ('clf', LogisticRegression(random_state = 111))\n",
    "#])\n",
    "model = LogisticRegression(random_state = 111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9310569544610328\n",
      "0.9306152248094507\n",
      "0.9305421410467266\n",
      "0.9307824632379182\n",
      "0.9303471877368105\n",
      "0.9301826068997148\n",
      "0.9305286632619125\n",
      "0.9305134770255022\n",
      "0.9307084303354185\n",
      "0.9305098702943547\n"
     ]
    }
   ],
   "source": [
    "coefs = np.empty((X.shape[1],))\n",
    "\n",
    "X_train, X_train_test, y_train, y_train_test = train_test_split(X, y, test_size=0.3, random_state=111)\n",
    "\n",
    "scores = []\n",
    "\n",
    "c_range = np.arange(1, 100, 10)\n",
    "for C in c_range:\n",
    "#    model = Pipeline([\n",
    "#        ('scaler',StandardScaler()),\n",
    "#        ('clf', LogisticRegression(penalty='l1', fit_intercept=True, C=C))\n",
    "#    ])\n",
    "\n",
    "    model = LogisticRegression(penalty='l1', fit_intercept=True, C=C)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    #coefs = np.c_[coefs, model.named_steps['clf'].coef_[0]]\n",
    "    s = roc_auc_score(y_train_test, model.predict_proba(X_train_test)[:, 1])\n",
    "    print(s)\n",
    "    scores.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.8, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 =  LogisticRegression(penalty='l1', fit_intercept=True, C=0.8)\n",
    "\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model2.predict_proba(X_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9310592323964944"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_train_test, y_predict[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = KFold(len(y_train), n_folds=5, shuffle=True, random_state=777)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:422: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max auc_roc: 0.9359021077797\n"
     ]
    }
   ],
   "source": [
    " searchCV = LogisticRegressionCV(\n",
    "        Cs=list(np.power(10.0, np.arange(-10, 10)))\n",
    "        ,penalty='l2'\n",
    "        ,scoring='roc_auc'\n",
    "        ,cv=fold\n",
    "        ,random_state=777\n",
    "        ,max_iter=10000\n",
    "        ,fit_intercept=True\n",
    "        ,solver='newton-cg'\n",
    "        ,tol=1e-4\n",
    "        ,class_weight='balanced'\n",
    "    )\n",
    "\n",
    "#sc = StandardScaler()\n",
    "#X_train_st = sc.fit_transform(X_train)\n",
    "#searchCV.fit(X_train_st, y_train)\n",
    "searchCV.fit(X_train, y_train)\n",
    "\n",
    "print ('Max auc_roc:', searchCV.scores_[1].mean(axis=0).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pr = searchCV.predict_proba(X_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9342980769668834"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_test_st = sc.transform(X_train_test)\n",
    "y_pr_st = searchCV.predict_proba(X_train_test_st)\n",
    "roc_auc_score(y_train_test, y_pr_st[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9343299680633448"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_train_test, y_pr[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_doc['target'] = searchCV.predict_proba(X_test)[:, 1]\n",
    "final_doc.to_csv('subm2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_train = pd.read_csv('train.csv')\n",
    "X_test = pd.read_csv('test.csv')\n",
    "\n",
    "# подгтовка train сета. удаляем id и раскладываем остальные текстовые признаки\n",
    "comp_train = comp_train.drop(['_id'], axis=1)\n",
    "comp_train = pd.get_dummies(comp_train, columns=['job', 'marital', 'default', 'education', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome'])\n",
    "# сохраняем id в отдельной таблице файле для сабмишна и также удаляем лишнее и раскладываем\n",
    "final_doc = pd.DataFrame()\n",
    "final_doc['_id'] = X_test['_id'] \n",
    "X_test = X_test.drop(['_id'], axis=1)\n",
    "X_test = pd.get_dummies(X_test, columns=['job', 'marital', 'default', 'education', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome'])\n",
    "\n",
    "X_test = X_test.iloc[:,[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 18, 22, 41, 42, 43, 44, 53, 55, 60, 61, 62]]\n",
    "\n",
    "X_test.info()\n",
    "\n",
    "# стандартно подготавливаем x и y\n",
    "y = comp_train['target']\n",
    "X = comp_train.drop(['target'], axis=1)\n",
    "X = X.iloc[:,[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 18, 22, 41, 42, 43, 44, 53, 55, 60, 61, 62]]\n",
    "\n",
    "# доп обработка для приведения всех параметров к одному разряду подрядку (-1 до 1)\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "stSc = StandardScaler()\n",
    "# X_test1 = stSc.fit_transform(X_test)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "X_train, X_train_test, y_train, y_train_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_hat = model.predict(X_train_test)\n",
    "\n",
    "y_hat_proba = model.predict_proba(X_train_test)\n",
    "\n",
    "y_hat_proba[:10]\n",
    "\n",
    "roc_auc_score(y_train_test, y_hat_proba[:, 1])\n",
    "\n",
    "scores_test = []\n",
    "scores_train = []\n",
    "\n",
    "for k in range(110, 200, 10):\n",
    "    %%time\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=k, n_jobs=-1, algorithm = 'kd_tree'))\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_hat_test = model.predict_proba(X_train_test)\n",
    "    scores_test.append(roc_auc_score(y_train_test, y_hat_test[:, 1]))\n",
    "    \n",
    "    y_hat_train = model.predict_proba(X_train)\n",
    "    scores_train.append(roc_auc_score(y_train, y_hat_train[:, 1]))\n",
    "        \n",
    "  #  print(k)\n",
    "\n",
    "plt.figure(figsize=(25,15))\n",
    "plt.plot(scores_test, label='test')\n",
    "plt.plot(scores_train, label='train')\n",
    "plt.legend()\n",
    "\n",
    "for p in range(1, 6, 1):\n",
    "    %%time\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=160, p=p, n_jobs=-1))\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_hat_test = model.predict_proba(X_train_test)\n",
    "    scores_test.append(roc_auc_score(y_train_test, y_hat_test[:, 1]))\n",
    "    \n",
    "    y_hat_train = model.predict_proba(X_train)\n",
    "    scores_train.append(roc_auc_score(y_train, y_hat_train[:, 1]))\n",
    "\n",
    "scores_test\n",
    "\n",
    "plt.figure(figsize=(25,15))\n",
    "plt.plot(scores_test[15:19], label='test')\n",
    "plt.plot(scores_train[15:19], label='train')\n",
    "plt.legend()\n",
    "\n",
    "model2 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=160, p=1))\n",
    "])\n",
    "\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "#y_hat = model2.predict(X_train_test)\n",
    "y_hat_proba = model2.predict_proba(X_train_test)\n",
    "roc_auc_score(y_train_test, y_hat_proba[:, 1])\n",
    "\n",
    "0.922640333627411\n",
    "\n",
    "final_doc['target'] = model2.predict_proba(X_test)[:, 1]\n",
    "final_doc.to_csv('submission2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
